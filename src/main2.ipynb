{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380de283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    brier_score_loss,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ปรับ path ตรงนี้ให้ตรงเครื่องคุณ\n",
    "DATA_DIR = \"/Users/wysuttida/pattern-project/mockdata_transaction\"\n",
    "ARTIFACT_DIR_API = \"/Users/wysuttida/pattern-project/API-Statement-IntelliScan\"  # สำหรับ export preprocessors / Keras\n",
    "os.makedirs(ARTIFACT_DIR_API, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60f86db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (10273, 33)\n"
     ]
    }
   ],
   "source": [
    "# %% [2] Loaders\n",
    "# โหลดไฟล์ csv/xlsx ทั้งโฟลเดอร์ พร้อมตั้งคอลัมน์ file_id เพื่อใช้ทำ group split\n",
    "\n",
    "def load_all_statements(data_dir: str = DATA_DIR) -> pd.DataFrame:\n",
    "    paths = sorted(\n",
    "        glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
    "        + glob.glob(os.path.join(data_dir, \"*.xlsx\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"ไม่พบไฟล์ใน {data_dir}\")\n",
    "\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        ext = os.path.splitext(p)[1].lower()\n",
    "        df = pd.read_excel(p, engine=\"openpyxl\") if ext == \".xlsx\" else pd.read_csv(p)\n",
    "        df[\"file_id\"] = os.path.basename(p)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "raw_df = load_all_statements(DATA_DIR)\n",
    "print(\"Loaded shape:\", raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d81bfe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed shape: (10273, 44)\n",
      "Class balance:\n",
      " fraud_label\n",
      "0    7881\n",
      "1    2392\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 0.2328\n"
     ]
    }
   ],
   "source": [
    "# %% [3] Preprocess\n",
    "# แปลง datetime, สร้างฟีเจอร์ numeric/time, แยก code/channel, ทำความสะอาด text/label\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # 3.1 datetime\n",
    "    df[\"tx_datetime\"] = pd.to_datetime(df[\"tx_datetime\"], errors=\"coerce\")\n",
    "    df = (\n",
    "        df.dropna(subset=[\"tx_datetime\"])\n",
    "        .sort_values([\"file_id\", \"tx_datetime\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # 3.2 split code/channel\n",
    "    sp = df[\"code_channel_raw\"].astype(str).str.split(\"/\", n=1, expand=True)\n",
    "    df[\"tx_code\"] = sp[0].str.strip()\n",
    "    df[\"channel\"] = sp[1].str.strip() if sp.shape[1] > 1 else \"\"\n",
    "\n",
    "    # 3.3 numeric ensure + engineer\n",
    "    for col in [\"debit_amount\", \"credit_amount\", \"balance_amount\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    df[\"net_amount\"] = df[\"credit_amount\"] - df[\"debit_amount\"]\n",
    "    df[\"abs_amount\"] = df[\"debit_amount\"].abs() + df[\"credit_amount\"].abs()\n",
    "    df[\"log1p_amount\"] = np.log1p(df[\"abs_amount\"])\n",
    "\n",
    "    # 3.4 time features\n",
    "    dt = df[\"tx_datetime\"]\n",
    "    df[\"hour\"] = dt.dt.hour\n",
    "    df[\"dayofweek\"] = dt.dt.dayofweek\n",
    "    df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "    df[\"day\"] = dt.dt.day\n",
    "    df[\"month\"] = dt.dt.month\n",
    "    df[\"year\"] = dt.dt.year\n",
    "\n",
    "    # 3.5 text & label\n",
    "    df[\"description_text\"] = df[\"description_text\"].astype(str).fillna(\"\")\n",
    "    df[\"fraud_label\"] = df[\"fraud_label\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = preprocess_dataframe(raw_df)\n",
    "print(\"Preprocessed shape:\", df.shape)\n",
    "print(\"Class balance:\\n\", df[\"fraud_label\"].value_counts(dropna=False))\n",
    "print(\"Positive rate:\", round(df[\"fraud_label\"].mean(), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a17d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes => train: 8309, val: 862, test: 1102\n"
     ]
    }
   ],
   "source": [
    "# %% [4] Split (Group by file_id)\n",
    "# แบ่ง 80/10/10 โดยใช้ GroupShuffleSplit เพื่อให้ไฟล์เดียวกันไม่ข้ามชุด\n",
    "\n",
    "groups = df[\"file_id\"].values\n",
    "y_all = df[\"fraud_label\"].values\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, temp_idx = next(gss.split(df, y_all, groups=groups))\n",
    "\n",
    "groups_temp = groups[temp_idx]\n",
    "y_temp = y_all[temp_idx]\n",
    "gss2 = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=42)\n",
    "val_rel, test_rel = next(gss2.split(df.iloc[temp_idx], y_temp, groups=groups_temp))\n",
    "\n",
    "val_idx = temp_idx[val_rel]\n",
    "test_idx = temp_idx[test_rel]\n",
    "\n",
    "def take(idx):\n",
    "    return df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "train_df, val_df, test_df = take(train_idx), take(val_idx), take(test_idx)\n",
    "print(f\"Split sizes => train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83e67206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [5] Feature Columns & XY helpers\n",
    "# นิยามคอลัมน์และฟังก์ชันดึง X/y/groups\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"debit_amount\",\n",
    "    \"credit_amount\",\n",
    "    \"balance_amount\",\n",
    "    \"net_amount\",\n",
    "    \"abs_amount\",\n",
    "    \"log1p_amount\",\n",
    "    \"hour\",\n",
    "    \"dayofweek\",\n",
    "    \"is_weekend\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\"tx_code\", \"channel\"]\n",
    "TEXT_FEATURE = \"description_text\"\n",
    "\n",
    "def xy(df_: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "    X = df_[\n",
    "        NUMERIC_FEATURES + CATEGORICAL_FEATURES + [TEXT_FEATURE]\n",
    "    ].copy()\n",
    "    y = df_[\"fraud_label\"].values\n",
    "    groups = df_[\"file_id\"].values\n",
    "    return X, y, groups\n",
    "\n",
    "X_train, y_train, g_train = xy(train_df)\n",
    "X_val, y_val, g_val = xy(val_df)\n",
    "X_test, y_test, g_test = xy(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d370f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [6] Preprocessor (ColumnTransformer)\n",
    "# หมายเหตุ: หาก scikit-learn <1.2 ให้เปลี่ยน OneHotEncoder(sparse_output=True) เป็น OneHotEncoder(sparse=True)\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler(with_mean=True, with_std=True))])\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "text_transformer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, NUMERIC_FEATURES),\n",
    "        (\"cat\", categorical_transformer, CATEGORICAL_FEATURES),\n",
    "        (\"txt\", text_transformer, TEXT_FEATURE),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "466e11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [7] Metrics helpers (score + threshold tuning + report)\n",
    "# รวมทุกเมตริกที่ใช้บ่อย พร้อมเลือก threshold จาก validation ตาม metric ที่สนใจ\n",
    "\n",
    "def _scores_from_estimator(clf, X):\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        return clf.predict_proba(X)[:, 1]\n",
    "    # fallback decision_function → scale เข้า [0,1]\n",
    "    d = clf.decision_function(X)\n",
    "    return (d - d.min()) / (d.max() - d.min() + 1e-9)\n",
    "\n",
    "def get_scores(clf, X, y, threshold=0.5) -> Dict:\n",
    "    scores = _scores_from_estimator(clf, X)\n",
    "    y_pred = (scores >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0, 1]).ravel()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y, y_pred),\n",
    "        \"precision\": precision_score(y, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y, scores) if len(np.unique(y)) > 1 else np.nan,\n",
    "        \"pr_auc(AP)\": average_precision_score(y, scores),\n",
    "        \"brier\": brier_score_loss(y, scores),\n",
    "        \"mcc\": matthews_corrcoef(y, y_pred) if len(np.unique(y_pred)) > 1 else 0.0,\n",
    "        \"tp\": tp,\n",
    "        \"fp\": fp,\n",
    "        \"tn\": tn,\n",
    "        \"fn\": fn,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "def find_best_threshold(clf, X_val, y_val, target_metric=\"accuracy\"):\n",
    "    scores = _scores_from_estimator(clf, X_val)\n",
    "    thresholds = np.linspace(0.05, 0.95, 19)\n",
    "    best_t, best_v = 0.5, -1.0\n",
    "    for t in thresholds:\n",
    "        pred = (scores >= t).astype(int)\n",
    "        if target_metric == \"f1\":\n",
    "            v = f1_score(y_val, pred, zero_division=0)\n",
    "        elif target_metric == \"recall\":\n",
    "            v = recall_score(y_val, pred, zero_division=0)\n",
    "        elif target_metric == \"precision\":\n",
    "            v = precision_score(y_val, pred, zero_division=0)\n",
    "        elif target_metric == \"accuracy\":\n",
    "            v = accuracy_score(y_val, pred)\n",
    "        else:\n",
    "            v = f1_score(y_val, pred, zero_division=0)\n",
    "        if v > best_v:\n",
    "            best_v, best_t = v, float(t)\n",
    "    return best_t, best_v\n",
    "\n",
    "def report_model(name, clf, X_tr, y_tr, X_va, y_va, X_te, y_te, tune_threshold=True, target_metric=\"accuracy\"):\n",
    "    thr = 0.5\n",
    "    if tune_threshold:\n",
    "        thr, _ = find_best_threshold(clf, X_va, y_va, target_metric=target_metric)\n",
    "    res_tr = get_scores(clf, X_tr, y_tr, threshold=thr)\n",
    "    res_va = get_scores(clf, X_va, y_va, threshold=thr)\n",
    "    res_te = get_scores(clf, X_te, y_te, threshold=thr)\n",
    "    df_res = pd.DataFrame([res_tr, res_va, res_te], index=[\"train\", \"val\", \"test\"])\n",
    "    df_res.insert(0, \"model\", name)\n",
    "    return df_res, thr\n",
    "\n",
    "def print_full_report(name, clf, X, y, threshold):\n",
    "    scores = _scores_from_estimator(clf, X)\n",
    "    pred = (scores >= threshold).astype(int)\n",
    "    print(f\"\\n{name} classification_report:\\n\", classification_report(y, pred, digits=4))\n",
    "    print(\"Confusion matrix [0,1]:\\n\", confusion_matrix(y, pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af19121e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Dummy baseline ==\n",
      "       model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train  Dummy  0.643519   0.230729  0.225026  0.227842  0.498095    0.233049   \n",
      "val    Dummy  0.660093   0.215686  0.248588  0.230971  0.507505    0.207909   \n",
      "test   Dummy  0.639746   0.263359  0.252747  0.257944  0.509968    0.251681   \n",
      "\n",
      "          brier       mcc   tp    fp    tn    fn  threshold  \n",
      "train  0.356481 -0.003844  437  1457  4910  1505        0.5  \n",
      "val    0.339907  0.014266   44   160   525   133        0.5  \n",
      "test   0.360254  0.020217   69   193   636   204        0.5  \n"
     ]
    }
   ],
   "source": [
    "# %% [8] Baseline (Dummy)\n",
    "pipe_dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "pipe_dummy.fit(X_train, y_train)\n",
    "dummy_res, dummy_thr = report_model(\n",
    "    \"Dummy\", pipe_dummy, X_train, y_train, X_val, y_val, X_test, y_test, tune_threshold=False\n",
    ")\n",
    "print(\"\\n== Dummy baseline ==\")\n",
    "print(dummy_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "450e3ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LR params: {'clf__C': 5.0, 'clf__penalty': 'l2', 'prep__txt__max_features': 2000, 'prep__txt__min_df': 3, 'prep__txt__ngram_range': (1, 2)}  CV(acc): 0.8127\n",
      "        model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train  LogReg  0.814659   0.758355  0.303811  0.433824  0.787100    0.613695   \n",
      "val    LogReg  0.837587   0.760563  0.305085  0.435484  0.770844    0.583992   \n",
      "test   LogReg  0.791289   0.741573  0.241758  0.364641  0.812670    0.606466   \n",
      "\n",
      "          brier       mcc   tp   fp    tn    fn  threshold  \n",
      "train  0.135718  0.398451  590  188  6179  1352        0.5  \n",
      "val    0.126111  0.411800   54   17   668   123        0.5  \n",
      "test   0.142106  0.339079   66   23   806   207        0.5  \n"
     ]
    }
   ],
   "source": [
    "# %% [9] Logistic Regression (GridSearchCV, scoring=accuracy)\n",
    "pipe_lr = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=500, solver=\"saga\", n_jobs=-1, class_weight=None)),\n",
    "])\n",
    "\n",
    "param_lr = {\n",
    "    \"prep__txt__max_features\": [2000, 5000, 10000],\n",
    "    \"prep__txt__min_df\": [3, 5],\n",
    "    \"prep__txt__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__C\": [0.5, 1.0, 2.0, 5.0],\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "}\n",
    "\n",
    "cv_lr = GridSearchCV(\n",
    "    pipe_lr,\n",
    "    param_grid=param_lr,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")\n",
    "cv_lr.fit(X_train, y_train, groups=g_train)\n",
    "best_lr = cv_lr.best_estimator_\n",
    "print(\"\\nBest LR params:\", cv_lr.best_params_, \" CV(acc):\", round(cv_lr.best_score_, 4))\n",
    "\n",
    "lr_res, lr_thr = report_model(\"LogReg\", best_lr, X_train, y_train, X_val, y_val, X_test, y_test, True, \"accuracy\")\n",
    "print(lr_res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad0fc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RF params: {'clf__max_depth': 30, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 3, 'prep__txt__max_features': 2000, 'prep__txt__min_df': 3, 'prep__txt__ngram_range': (1, 2)}  CV(acc): 0.8165\n",
      "              model  accuracy  precision    recall        f1   roc_auc  \\\n",
      "train  RandomForest  0.857504   0.990933  0.393924  0.563744  0.984404   \n",
      "val    RandomForest  0.836427   0.875000  0.237288  0.373333  0.832430   \n",
      "test   RandomForest  0.782214   0.753846  0.179487  0.289941  0.787899   \n",
      "\n",
      "       pr_auc(AP)     brier       mcc   tp  fp    tn    fn  threshold  \n",
      "train    0.948008  0.071376  0.572641  765   7  6360  1177        0.6  \n",
      "val      0.609913  0.119182  0.402569   42   6   679   135        0.6  \n",
      "test     0.591206  0.146055  0.293521   49  16   813   224        0.6  \n"
     ]
    }
   ],
   "source": [
    "# %% [10] RandomForest (GridSearchCV, scoring=accuracy)\n",
    "pipe_rf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=600, n_jobs=-1, class_weight=None, random_state=42)),\n",
    "])\n",
    "\n",
    "param_rf = {\n",
    "    \"prep__txt__max_features\": [2000, 5000, 10000],\n",
    "    \"prep__txt__min_df\": [3, 5],\n",
    "    \"prep__txt__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__max_depth\": [None, 20, 30],\n",
    "    \"clf__min_samples_leaf\": [1, 3, 5],\n",
    "    \"clf__max_features\": [\"sqrt\", 0.5, None],\n",
    "}\n",
    "\n",
    "cv_rf = GridSearchCV(\n",
    "    pipe_rf,\n",
    "    param_grid=param_rf,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")\n",
    "cv_rf.fit(X_train, y_train, groups=g_train)\n",
    "best_rf = cv_rf.best_estimator_\n",
    "print(\"\\nBest RF params:\", cv_rf.best_params_, \" CV(acc):\", round(cv_rf.best_score_, 4))\n",
    "\n",
    "rf_res, rf_thr = report_model(\"RandomForest\", best_rf, X_train, y_train, X_val, y_val, X_test, y_test, True, \"accuracy\")\n",
    "print(rf_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "559015c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MLP params: {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__batch_size': 512, 'clf__hidden_layer_sizes': (256,), 'clf__learning_rate_init': 0.003, 'clf__solver': 'adam', 'prep__txt__max_features': 2000, 'prep__txt__min_df': 3, 'prep__txt__ngram_range': (1, 1)}  CV(acc): 0.8273\n",
      "      model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train   MLP  0.843302   0.864465  0.390834  0.538298  0.875452    0.743055   \n",
      "val     MLP  0.853828   0.814815  0.372881  0.511628  0.822153    0.650232   \n",
      "test    MLP  0.792196   0.761905  0.234432  0.358543  0.802874    0.603220   \n",
      "\n",
      "          brier       mcc   tp   fp    tn    fn  threshold  \n",
      "train  0.109285  0.512309  759  119  6248  1183       0.55  \n",
      "val    0.113669  0.485904   66   15   670   111       0.55  \n",
      "test   0.145269  0.342136   64   20   809   209       0.55  \n"
     ]
    }
   ],
   "source": [
    "# %% [11] MLP (GridSearchCV, scoring=accuracy)\n",
    "pipe_mlp = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", MLPClassifier(\n",
    "        max_iter=600,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=8,\n",
    "        validation_fraction=0.1,\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_mlp = {\n",
    "    \"prep__txt__max_features\": [2000, 5000, 10000],\n",
    "    \"prep__txt__min_df\": [3, 5],\n",
    "    \"prep__txt__ngram_range\": [(1,1), (1,2)],\n",
    "    \"clf__hidden_layer_sizes\": [(128,), (256,), (256,128)],\n",
    "    \"clf__alpha\": [1e-4, 1e-3],\n",
    "    \"clf__learning_rate_init\": [1e-3, 3e-3],\n",
    "    \"clf__batch_size\": [256, 512],\n",
    "    \"clf__activation\": [\"relu\"],\n",
    "    \"clf__solver\": [\"adam\"],\n",
    "}\n",
    "\n",
    "cv_mlp = GridSearchCV(\n",
    "    pipe_mlp,\n",
    "    param_grid=param_mlp,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")\n",
    "cv_mlp.fit(X_train, y_train, groups=g_train)\n",
    "best_mlp = cv_mlp.best_estimator_\n",
    "print(\"\\nBest MLP params:\", cv_mlp.best_params_, \" CV(acc):\", round(cv_mlp.best_score_, 4))\n",
    "\n",
    "mlp_res, mlp_thr = report_model(\"MLP\", best_mlp, X_train, y_train, X_val, y_val, X_test, y_test, True, \"accuracy\")\n",
    "print(mlp_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a24444d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present result frames: {'dummy_res': True, 'lr_res': True, 'rf_res': False, 'mlp_res': True}\n",
      "\n",
      "== Validation Summary (sorted by PR-AUC) ==\n",
      "        pr_auc(AP)        f1    recall\n",
      "model                                 \n",
      "MLP       0.650232  0.511628  0.372881\n",
      "LogReg    0.583992  0.435484  0.305085\n",
      "Dummy     0.207909  0.230971  0.248588\n",
      "winner_name = MLP\n",
      "\n",
      "== FINAL on TEST using MLP (thr=0.30) ==\n",
      "      accuracy  precision    recall        f1  roc_auc  pr_auc(AP)     brier  \\\n",
      "test  0.777677   0.571429  0.410256  0.477612  0.77714      0.5783  0.152842   \n",
      "\n",
      "           mcc   tp  fp   tn   fn  threshold  \n",
      "test  0.348759  112  84  745  161        0.3  \n",
      "\n",
      "BEST(TEST) classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8223    0.8987    0.8588       829\n",
      "           1     0.5714    0.4103    0.4776       273\n",
      "\n",
      "    accuracy                         0.7777      1102\n",
      "   macro avg     0.6969    0.6545    0.6682      1102\n",
      "weighted avg     0.7601    0.7777    0.7644      1102\n",
      "\n",
      "Confusion matrix [0,1]:\n",
      " [[745  84]\n",
      " [161 112]]\n"
     ]
    }
   ],
   "source": [
    "# %% [12] Pick Winner on Validation & Refit on Train+Val, Evaluate on Test\n",
    "present = {vn: isinstance(globals().get(vn), pd.DataFrame) for vn in [\"dummy_res\", \"lr_res\", \"rf_res\", \"mlp_res\"]}\n",
    "print(\"present result frames:\", present)\n",
    "\n",
    "frames = []\n",
    "for label, varname in [(\"Dummy\", \"dummy_res\"), (\"LogReg\", \"lr_res\"), (\"RandomForest\", \"rf_res\"), (\"MLP\", \"mlp_res\")]:\n",
    "    df_var = globals().get(varname)\n",
    "    if isinstance(df_var, pd.DataFrame) and \"val\" in df_var.index:\n",
    "        frames.append(df_var.loc[[\"val\"]].copy())\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No model validation results found.\")\n",
    "\n",
    "summary_val = pd.concat(frames, axis=0).set_index(\"model\", drop=True)\n",
    "summary_val_no_dummy = summary_val.drop(index=\"Dummy\", errors=\"ignore\")\n",
    "candidates = summary_val_no_dummy if len(summary_val_no_dummy) > 0 else summary_val\n",
    "\n",
    "winner_name = candidates.sort_values(by=[\"pr_auc(AP)\", \"f1\", \"recall\"], ascending=False).index[0]\n",
    "print(\"\\n== Validation Summary (sorted by PR-AUC) ==\")\n",
    "print(summary_val.loc[:, [\"pr_auc(AP)\", \"f1\", \"recall\"]].sort_values(\"pr_auc(AP)\", ascending=False))\n",
    "print(\"winner_name =\", winner_name)\n",
    "\n",
    "winners = {}\n",
    "if \"LogReg\" in summary_val.index:       winners[\"LogReg\"] = (best_lr, lr_thr)\n",
    "if \"RandomForest\" in summary_val.index:  winners[\"RandomForest\"] = (best_rf, rf_thr)\n",
    "if \"MLP\" in summary_val.index:           winners[\"MLP\"] = (best_mlp, mlp_thr)\n",
    "\n",
    "best_model_template, _ = winners.get(winner_name, (None, None))\n",
    "if best_model_template is None:\n",
    "    raise RuntimeError(f\"Winner '{winner_name}' not available in winners map.\")\n",
    "\n",
    "# concat X for train+val\n",
    "def concat_X(*dfs):\n",
    "    return pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "X_trval = concat_X(X_train, X_val)\n",
    "y_trval = np.concatenate([y_train, y_val])\n",
    "\n",
    "best_model_final = best_model_template\n",
    "best_model_final.fit(X_trval, y_trval)\n",
    "\n",
    "# เลือก threshold จาก validation โดย optimize F1 (ปรับได้)\n",
    "final_thr, _ = find_best_threshold(best_model_final, X_val, y_val, target_metric=\"f1\")\n",
    "final_res = get_scores(best_model_final, X_test, y_test, threshold=final_thr)\n",
    "\n",
    "print(f\"\\n== FINAL on TEST using {winner_name} (thr={final_thr:.2f}) ==\")\n",
    "print(pd.DataFrame([final_res], index=[\"test\"]))\n",
    "print_full_report(\"BEST(TEST)\", best_model_final, X_test, y_test, final_thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da96a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP has no direct coefficients — running permutation importance...\n",
      "\n",
      "Top 15 permutation importances (input level):\n",
      "log1p_amount 0.32219\n",
      "channel 0.248087\n",
      "tx_code 0.229019\n",
      "description_text 0.162943\n",
      "credit_amount 0.061991\n",
      "debit_amount 0.027332\n",
      "hour 0.026856\n",
      "balance_amount 0.02416\n",
      "abs_amount 0.021441\n",
      "dayofweek 0.019027\n",
      "net_amount 0.01681\n",
      "month 0.008305\n",
      "is_weekend 0.006585\n",
      "year 0.00576\n",
      "day -0.001862\n"
     ]
    }
   ],
   "source": [
    "# %% [13] Feature Importance / Interpretability\n",
    "# สำหรับ LogReg แสดง coefficients, RF แสดง feature_importances_, อื่น ๆ ใช้ permutation_importance\n",
    "\n",
    "def get_feature_names(ct: ColumnTransformer):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name == \"num\":\n",
    "            names.extend(cols)\n",
    "        elif name == \"cat\":\n",
    "            ohe = trans\n",
    "            try:\n",
    "                names.extend(list(ohe.get_feature_names_out(cols)))\n",
    "            except Exception:\n",
    "                names.extend(cols)\n",
    "        elif name == \"txt\":\n",
    "            tfidf = trans\n",
    "            try:\n",
    "                names.extend(list(tfidf.get_feature_names_out()))\n",
    "            except Exception:\n",
    "                names.append(\"tfidf_features\")\n",
    "    return names\n",
    "\n",
    "if winner_name == \"LogReg\":\n",
    "    coefs = best_model_final.named_steps[\"clf\"].coef_[0]\n",
    "    feat_names = get_feature_names(best_model_final.named_steps[\"prep\"])\n",
    "    top_pos_idx = np.argsort(coefs)[-15:][::-1]\n",
    "    top_neg_idx = np.argsort(coefs)[:15]\n",
    "    print(\"\\nTop + coefficients:\")\n",
    "    for i in top_pos_idx:\n",
    "        print(feat_names[i], round(coefs[i], 4))\n",
    "    print(\"\\nTop - coefficients:\")\n",
    "    for i in top_neg_idx:\n",
    "        print(feat_names[i], round(coefs[i], 4))\n",
    "\n",
    "elif winner_name == \"RandomForest\":\n",
    "    rf = best_model_final.named_steps[\"clf\"]\n",
    "    feat_names = get_feature_names(best_model_final.named_steps[\"prep\"])\n",
    "    importances = getattr(rf, \"feature_importances_\", None)\n",
    "    if importances is not None:\n",
    "        idx = np.argsort(importances)[-20:][::-1]\n",
    "        print(\"\\nTop 20 RF importances:\")\n",
    "        for i in idx:\n",
    "            print(feat_names[i], round(importances[i], 4))\n",
    "\n",
    "else:\n",
    "    print(f\"{winner_name} has no direct coefficients — running permutation importance...\")\n",
    "    r = permutation_importance(\n",
    "        best_model_final,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        n_repeats=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        scoring=\"average_precision\",\n",
    "    )\n",
    "    base_names = list(X_test.columns)\n",
    "    idx = np.argsort(r.importances_mean)[-15:][::-1]\n",
    "    print(\"\\nTop 15 permutation importances (input level):\")\n",
    "    for i in idx:\n",
    "        print(base_names[i], round(r.importances_mean[i], 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65b4dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %% [14] (Optional) Install extras for Excel ifจำเป็น\n",
    "# ใช้เฉพาะกรณี Notebook ยังไม่มี openpyxl\n",
    "%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc91ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved scaler/tfidf/vocabs to /Users/wysuttida/pattern-project/API-Statement-IntelliScan\n"
     ]
    }
   ],
   "source": [
    "# %% [15] External Preprocessors Export (scaler / tfidf / vocab)  → ใช้คู่กับ API\n",
    "# กรณีที่ฝั่ง API ต้องการโหลดสเกลเลอร์/TF-IDF/หมวดหมู่ เพื่อแปลงฟีเจอร์ภายนอกโมเดล Keras\n",
    "\n",
    "import joblib\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# 15.1 Fit preprocessors จาก train_df\n",
    "scaler = StandardScaler(with_mean=True, with_std=True).fit(train_df[NUMERIC_FEATURES])\n",
    "\n",
    "tx_vocab = sorted(train_df[\"tx_code\"].astype(str).unique().tolist())\n",
    "ch_vocab = sorted(train_df[\"channel\"].astype(str).unique().tolist())\n",
    "tx_index = {t: i for i, t in enumerate(tx_vocab)}\n",
    "ch_index = {t: i for i, t in enumerate(ch_vocab)}\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    strip_accents=\"unicode\",\n",
    ").fit(train_df[TEXT_FEATURE].astype(str))\n",
    "\n",
    "# 15.2 Save\n",
    "joblib.dump(scaler, f\"{ARTIFACT_DIR_API}/pre_scaler.joblib\")\n",
    "joblib.dump(tfidf, f\"{ARTIFACT_DIR_API}/pre_tfidf.joblib\")\n",
    "with open(f\"{ARTIFACT_DIR_API}/pre_categ_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"tx_vocab\": tx_vocab, \"ch_vocab\": ch_vocab}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] Saved scaler/tfidf/vocabs to\", ARTIFACT_DIR_API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b18316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: train (8309, 201) val (862, 201) test (1102, 201)\n"
     ]
    }
   ],
   "source": [
    "# %% [16] Build Sparse Matrices (numeric + one-hot + tfidf)\n",
    "# แปลง DataFrame เป็น CSR Matrix เพื่อป้อนให้ Keras (หรือโมเดลอื่น ๆ)\n",
    "\n",
    "def _one_hot_from_vocab(series_str: pd.Series, index_map: dict, vocab_size: int):\n",
    "    arr = series_str.astype(str).map(index_map).to_numpy()\n",
    "    N = len(arr)\n",
    "    rows = np.arange(N, dtype=np.int64)\n",
    "    mask = ~pd.isna(arr)\n",
    "    cols = arr[mask].astype(np.int64)\n",
    "    data = np.ones(mask.sum(), dtype=np.float32)\n",
    "    return sp.csr_matrix((data, (rows[mask], cols)), shape=(N, vocab_size), dtype=np.float32)\n",
    "\n",
    "def transform_df_to_X(\n",
    "    df_: pd.DataFrame,\n",
    "    scaler_: StandardScaler,\n",
    "    tfidf_: TfidfVectorizer,\n",
    "    tx_index_: Dict[str, int],\n",
    "    ch_index_: Dict[str, int],\n",
    "    tx_vocab_: list,\n",
    "    ch_vocab_: list,\n",
    "):\n",
    "    # numeric → scale → csr\n",
    "    X_num = scaler_.transform(df_[NUMERIC_FEATURES]).astype(np.float32)\n",
    "    X_num = sp.csr_matrix(X_num)\n",
    "\n",
    "    # categorical → one-hot csr\n",
    "    X_tx = _one_hot_from_vocab(df_[\"tx_code\"].astype(str), tx_index_, len(tx_vocab_))\n",
    "    X_ch = _one_hot_from_vocab(df_[\"channel\"].astype(str), ch_index_, len(ch_vocab_))\n",
    "\n",
    "    # text → tfidf csr\n",
    "    X_txt = tfidf_.transform(df_[TEXT_FEATURE].astype(str)).astype(np.float32)\n",
    "\n",
    "    # hstack\n",
    "    return sp.hstack([X_num, X_tx, X_ch, X_txt], format=\"csr\", dtype=np.float32)\n",
    "\n",
    "X_train_ext = transform_df_to_X(train_df, scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "X_val_ext   = transform_df_to_X(val_df,   scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "X_test_ext  = transform_df_to_X(test_df,  scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "\n",
    "print(\"Shapes:\", \"train\", X_train_ext.shape, \"val\", X_val_ext.shape, \"test\", X_test_ext.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 01:01:23.682511: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2025-10-09 01:01:23.682671: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-09 01:01:23.682677: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-10-09 01:01:23.682868: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-09 01:01:23.683222: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-10-09 01:01:24.032299: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'accuracy': 0.779491833030853, 'precision': 0.5555555555555556, 'recall': 0.5494505494505495, 'f1': 0.5524861878453039, 'roc_auc': 0.8105975247109144, 'pr_auc(AP)': 0.60512966342085, 'threshold': 0.6}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OUT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m, final_metrics)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# เซฟเป็น .h5 ได้แล้ว (ไม่มี StringLookup/TextVectorization ภายในโมเดล)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_meta.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     89\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\n\u001b[1;32m     90\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_thr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics_test\u001b[39m\u001b[38;5;124m\"\u001b[39m: final_metrics},\n\u001b[1;32m     91\u001b[0m         f,\n\u001b[1;32m     92\u001b[0m         ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m         indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     94\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# ==== 16C) Keras model (no preprocessing inside) + save .h5 ====\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "# แปลงเป็น dense (ถ้าเมมไม่พอ ลดฟีเจอร์ก่อน)\n",
    "Xtr = X_train_ext.toarray()\n",
    "Xva = X_val_ext.toarray()\n",
    "Xte = X_test_ext.toarray()\n",
    "\n",
    "# Labels\n",
    "y_tr = y_train.astype(np.float32)\n",
    "y_va = y_val.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "# Class weights → sample_weight (เหมือนเดิม)\n",
    "classes = np.array([0, 1])\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "w0, w1 = float(cw[0]), float(cw[1])\n",
    "sw_tr = np.where(y_tr == 1, w1, w0).astype(np.float32)\n",
    "\n",
    "# Build model (input = เวกเตอร์ฟีเจอร์รวม)\n",
    "inp = layers.Input(shape=(Xtr.shape[1],), name=\"X\")\n",
    "x = layers.Dense(128, activation=\"relu\")(inp)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inp, out)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, restore_best_weights=True, monitor=\"val_auc\", mode=\"max\"\n",
    ")\n",
    "history = model.fit(\n",
    "    Xtr,\n",
    "    y_tr,\n",
    "    validation_data=(Xva, y_va),\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    sample_weight=sw_tr,\n",
    "    callbacks=[es],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# เลือก threshold จาก validation เพื่อ maximize F1\n",
    "val_scores = model.predict(Xva, verbose=0).ravel()\n",
    "ths = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "\n",
    "def f1_at(t):\n",
    "    return f1_score(y_va, (val_scores >= t).astype(int), zero_division=0)\n",
    "\n",
    "\n",
    "best_thr = float(ths[np.argmax([f1_at(t) for t in ths])])\n",
    "\n",
    "# ประเมินบน test\n",
    "test_scores = model.predict(Xte, verbose=0).ravel()\n",
    "pred_test = (test_scores >= best_thr).astype(int)\n",
    "final_metrics = {\n",
    "    \"accuracy\": float((pred_test == y_te).mean()),\n",
    "    \"precision\": float(precision_score(y_te, pred_test, zero_division=0)),\n",
    "    \"recall\": float(recall_score(y_te, pred_test, zero_division=0)),\n",
    "    \"f1\": float(f1_score(y_te, pred_test, zero_division=0)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, test_scores)),\n",
    "    \"pr_auc(AP)\": float(average_precision_score(y_te, test_scores)),\n",
    "    \"threshold\": best_thr,\n",
    "}\n",
    "print(\"Test metrics:\", final_metrics)\n",
    "\n",
    "# เซฟเป็น .h5 ได้แล้ว (ไม่มี StringLookup/TextVectorization ภายในโมเดล)\n",
    "model.save(f\"{ARTIFACT_DIR_API}/model.h5\")\n",
    "with open(f\"{ARTIFACT_DIR_API}/model_meta.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {\"threshold\": best_thr, \"metrics_test\": final_metrics},\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "print(\"[OK] Saved model.h5 and model_meta.json at\", ARTIFACT_DIR_API)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
