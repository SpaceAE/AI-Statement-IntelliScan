{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d60b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "\n",
    "import numpy as np, json, os\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "   accuracy_score, precision_score, recall_score, f1_score,\n",
    "   roc_auc_score, average_precision_score, confusion_matrix,\n",
    "   classification_report, matthews_corrcoef, brier_score_loss\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fe79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_statements(data_dir=\"/Users/wysuttida/pattern-project/mockdata_transaction\"):\n",
    "   \"\"\"\n",
    "   อ่านทุกไฟล์ .csv / .xlsx ในโฟลเดอร์ data/\n",
    "   ต้องมีคอลัมน์: tx_datetime, code_channel_raw, debit_amount, credit_amount, balance_amount, description_text, fraud_label\n",
    "   คืนค่า df รวมทุกไฟล์ พร้อมคอลัมน์ file_id (ชื่อไฟล์)\n",
    "   \"\"\"\n",
    "   paths = sorted(glob.glob(os.path.join(data_dir, \"*.csv\")) + glob.glob(os.path.join(data_dir, \"*.xlsx\")))\n",
    "   if not paths:\n",
    "       raise FileNotFoundError(\"ไม่พบไฟล์ในโฟลเดอร์ mockdata_transaction/\")\n",
    "\n",
    "\n",
    "   dfs = []\n",
    "   for p in paths:\n",
    "       ext = os.path.splitext(p)[1].lower()\n",
    "       if ext == \".csv\":\n",
    "           df = pd.read_csv(p)\n",
    "       else:\n",
    "           df = pd.read_excel(p, engine=\"openpyxl\")\n",
    "\n",
    "\n",
    "       df[\"file_id\"] = os.path.basename(p)  # ใช้เป็น group\n",
    "       dfs.append(df)\n",
    "\n",
    "\n",
    "   df = pd.concat(dfs, ignore_index=True)\n",
    "   return df\n",
    "\n",
    "\n",
    "df = load_all_statements(\"/Users/wysuttida/pattern-project/mockdata_transaction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2850be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "   # 2.1 datetime\n",
    "   df[\"tx_datetime\"] = pd.to_datetime(df[\"tx_datetime\"], errors=\"coerce\")\n",
    "   df = df.dropna(subset=[\"tx_datetime\"]).sort_values([\"file_id\",\"tx_datetime\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "   # 2.2 split code/channel\n",
    "   sp = df[\"code_channel_raw\"].astype(str).str.split(\"/\", n=1, expand=True)\n",
    "   df[\"tx_code\"] = sp[0].str.strip()\n",
    "   df[\"channel\"] = sp[1].str.strip() if sp.shape[1] > 1 else \"\"\n",
    "\n",
    "\n",
    "   # 2.3 amount features\n",
    "   for col in [\"debit_amount\",\"credit_amount\",\"balance_amount\"]:\n",
    "       if col not in df.columns:\n",
    "           df[col] = 0.0\n",
    "   df[\"net_amount\"] = df[\"credit_amount\"].fillna(0) - df[\"debit_amount\"].fillna(0)\n",
    "   df[\"abs_amount\"] = (df[\"debit_amount\"].fillna(0).abs() + df[\"credit_amount\"].fillna(0).abs())\n",
    "   df[\"log1p_amount\"] = np.log1p(df[\"abs_amount\"])\n",
    "\n",
    "\n",
    "   # 2.4 time features\n",
    "   dt = df[\"tx_datetime\"]\n",
    "   df[\"hour\"] = dt.dt.hour\n",
    "   df[\"dayofweek\"] = dt.dt.dayofweek     # 0=Mon\n",
    "   df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "   df[\"day\"] = dt.dt.day\n",
    "   df[\"month\"] = dt.dt.month\n",
    "   df[\"year\"] = dt.dt.year\n",
    "\n",
    "\n",
    "   # 2.5 text\n",
    "   df[\"description_text\"] = df[\"description_text\"].astype(str).fillna(\"\")\n",
    "\n",
    "\n",
    "   # 2.6 label\n",
    "   df[\"fraud_label\"] = df[\"fraud_label\"].astype(int)\n",
    "\n",
    "\n",
    "   return df\n",
    "\n",
    "\n",
    "df = preprocess_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9fce867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (10273, 44)\n",
      "Class balance (0/1):\n",
      "fraud_label\n",
      "0    7881\n",
      "1    2392\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 0.2328\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"Class balance (0/1):\")\n",
    "print(df[\"fraud_label\"].value_counts(dropna=False))\n",
    "print(\"Positive rate:\", df[\"fraud_label\"].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae9a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes => train: 8309, val: 862, test: 1102\n"
     ]
    }
   ],
   "source": [
    "groups = df[\"file_id\"].values\n",
    "y = df[\"fraud_label\"].values\n",
    "\n",
    "\n",
    "# แบ่ง 80% train vs 20% temp\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, temp_idx = next(gss.split(df, y, groups=groups))\n",
    "\n",
    "\n",
    "# แบ่ง temp (20%) เป็น val/test อย่างละครึ่ง\n",
    "groups_temp = groups[temp_idx]\n",
    "y_temp = y[temp_idx]\n",
    "gss2 = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=42)\n",
    "val_rel, test_rel = next(gss2.split(df.iloc[temp_idx], y_temp, groups=groups_temp))\n",
    "val_idx = temp_idx[val_rel]\n",
    "test_idx = temp_idx[test_rel]\n",
    "\n",
    "\n",
    "def take(idx):\n",
    "   return df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df = take(train_idx)\n",
    "val_df   = take(val_idx)\n",
    "test_df  = take(test_idx)\n",
    "\n",
    "\n",
    "def xy(df_):\n",
    "   X = df_[[\"debit_amount\",\"credit_amount\",\"balance_amount\",\"net_amount\",\"abs_amount\",\"log1p_amount\",\n",
    "            \"hour\",\"dayofweek\",\"is_weekend\",\"day\",\"month\",\"year\",\n",
    "            \"tx_code\",\"channel\",\"description_text\"]].copy()\n",
    "   y = df_[\"fraud_label\"].values\n",
    "   groups = df_[\"file_id\"].values\n",
    "   return X, y, groups\n",
    "\n",
    "\n",
    "X_train, y_train, g_train = xy(train_df)\n",
    "X_val,   y_val,   g_val   = xy(val_df)\n",
    "X_test,  y_test,  g_test  = xy(test_df)\n",
    "\n",
    "\n",
    "print(f\"Split sizes => train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab85460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "   \"debit_amount\",\"credit_amount\",\"balance_amount\",\"net_amount\",\"abs_amount\",\"log1p_amount\",\n",
    "   \"hour\",\"dayofweek\",\"is_weekend\",\"day\",\"month\",\"year\"\n",
    "]\n",
    "categorical_features = [\"tx_code\",\"channel\"]\n",
    "text_feature = \"description_text\"\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "   (\"scaler\", StandardScaler(with_mean=True, with_std=True))\n",
    "])\n",
    "\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "\n",
    "\n",
    "text_transformer = TfidfVectorizer(\n",
    "   ngram_range=(1,2), max_features=5000, min_df=5, max_df=0.95, strip_accents=\"unicode\"\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "       (\"num\", numeric_transformer, numeric_features),\n",
    "       (\"cat\", categorical_transformer, categorical_features),\n",
    "       (\"txt\", text_transformer, text_feature),\n",
    "   ],\n",
    "   remainder=\"drop\",\n",
    "   sparse_threshold=0.3,  # อนุญาต sparse\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e813af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(clf, X, y, threshold=0.5, average='binary'):\n",
    "   # รองรับทั้ง predict_proba/decision_function\n",
    "   if hasattr(clf, \"predict_proba\"):\n",
    "       s = clf.predict_proba(X)[:, 1]\n",
    "   else:\n",
    "       # scale decision_function ให้เข้า [0,1] แบบคร่าว ๆ\n",
    "       d = clf.decision_function(X)\n",
    "       s = (d - d.min()) / (d.max() - d.min() + 1e-9)\n",
    "\n",
    "\n",
    "   y_pred = (s >= threshold).astype(int)\n",
    "\n",
    "\n",
    "   cm = confusion_matrix(y, y_pred, labels=[0,1])\n",
    "   tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "\n",
    "   return {\n",
    "       \"accuracy\": accuracy_score(y, y_pred),\n",
    "       \"precision\": precision_score(y, y_pred, zero_division=0),\n",
    "       \"recall\": recall_score(y, y_pred, zero_division=0),\n",
    "       \"f1\": f1_score(y, y_pred, zero_division=0),\n",
    "       \"roc_auc\": roc_auc_score(y, s) if len(np.unique(y))>1 else np.nan,\n",
    "       \"pr_auc(AP)\": average_precision_score(y, s),\n",
    "       \"brier\": brier_score_loss(y, s),\n",
    "       \"mcc\": matthews_corrcoef(y, y_pred) if len(np.unique(y_pred))>1 else 0.0,\n",
    "       \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn,\n",
    "       \"threshold\": threshold,\n",
    "   }\n",
    "\n",
    "\n",
    "def find_best_threshold(clf, X_val, y_val, target_metric=\"f1\"):\n",
    "   # ทดลอง threshold 101 ค่า\n",
    "   if hasattr(clf, \"predict_proba\"):\n",
    "       s = clf.predict_proba(X_val)[:, 1]\n",
    "   else:\n",
    "       d = clf.decision_function(X_val)\n",
    "       s = (d - d.min()) / (d.max() - d.min() + 1e-9)\n",
    "\n",
    "\n",
    "   thresholds = np.linspace(0.05, 0.95, 19)\n",
    "   best_t, best_v = 0.5, -1\n",
    "   for t in thresholds:\n",
    "       y_pred = (s >= t).astype(int)\n",
    "       if target_metric == \"f1\":\n",
    "           v = f1_score(y_val, y_pred, zero_division=0)\n",
    "       elif target_metric == \"recall\":\n",
    "           v = recall_score(y_val, y_pred, zero_division=0)\n",
    "       elif target_metric == \"precision\":\n",
    "           v = precision_score(y_val, y_pred, zero_division=0)\n",
    "       else:\n",
    "           v = f1_score(y_val, y_pred, zero_division=0)\n",
    "       if v > best_v:\n",
    "           best_v, best_t = v, t\n",
    "   return best_t, best_v\n",
    "\n",
    "\n",
    "def report_model(name, clf, X_tr, y_tr, X_va, y_va, X_te, y_te, tune_threshold=True):\n",
    "   thr = 0.5\n",
    "   if tune_threshold:\n",
    "       thr, _ = find_best_threshold(clf, X_va, y_va, target_metric=\"f1\")\n",
    "   res_tr = get_scores(clf, X_tr, y_tr, threshold=thr)\n",
    "   res_va = get_scores(clf, X_va, y_va, threshold=thr)\n",
    "   res_te = get_scores(clf, X_te, y_te, threshold=thr)\n",
    "   df_res = pd.DataFrame([res_tr, res_va, res_te], index=[\"train\",\"val\",\"test\"])\n",
    "   df_res.insert(0, \"model\", name)\n",
    "   return df_res, thr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5224199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Dummy baseline ==\n",
      "       model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train  Dummy  0.643519   0.230729  0.225026  0.227842  0.498095    0.233049   \n",
      "val    Dummy  0.660093   0.215686  0.248588  0.230971  0.507505    0.207909   \n",
      "test   Dummy  0.639746   0.263359  0.252747  0.257944  0.509968    0.251681   \n",
      "\n",
      "          brier       mcc   tp    fp    tn    fn  threshold  \n",
      "train  0.356481 -0.003844  437  1457  4910  1505        0.5  \n",
      "val    0.339907  0.014266   44   160   525   133        0.5  \n",
      "test   0.360254  0.020217   69   193   636   204        0.5  \n"
     ]
    }
   ],
   "source": [
    "pipe_dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "pipe_dummy.fit(X_train, y_train)\n",
    "dummy_res, dummy_thr = report_model(\"Dummy\", pipe_dummy, X_train, y_train, X_val, y_val, X_test, y_test, tune_threshold=False)\n",
    "print(\"\\n== Dummy baseline ==\")\n",
    "print(dummy_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32605d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LR params: {'clf__C': 5.0, 'clf__penalty': 'l1'}  CV(AP): 0.5962\n",
      "        model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train  LogReg  0.786256   0.542828  0.541710  0.542268  0.786809    0.609055   \n",
      "val    LogReg  0.801624   0.517857  0.491525  0.504348  0.771199    0.582585   \n",
      "test   LogReg  0.781307   0.559259  0.553114  0.556169  0.813589    0.607523   \n",
      "\n",
      "          brier       mcc    tp   fp    tn   fn  threshold  \n",
      "train  0.183984  0.402843  1052  886  5481  890        0.6  \n",
      "val    0.181923  0.380653    87   81   604   90        0.6  \n",
      "test   0.179366  0.411093   151  119   710  122        0.6  \n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([\n",
    "   (\"prep\", preprocessor),\n",
    "   (\"clf\", LogisticRegression(\n",
    "       max_iter=1000, solver=\"saga\", n_jobs=-1, class_weight=\"balanced\"\n",
    "   )),\n",
    "])\n",
    "param_lr = {\n",
    "   \"clf__C\": [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "   \"clf__penalty\": [\"l1\",\"l2\"],  # saga รองรับ l1,l2\n",
    "}\n",
    "cv_lr = GridSearchCV(\n",
    "   pipe_lr, param_grid=param_lr, cv=GroupKFold(n_splits=5),\n",
    "   scoring=\"average_precision\", n_jobs=-1, verbose=0\n",
    ")\n",
    "cv_lr.fit(X_train, y_train, clf__sample_weight=None, groups=g_train)\n",
    "best_lr = cv_lr.best_estimator_\n",
    "print(\"\\nBest LR params:\", cv_lr.best_params_, \" CV(AP):\", round(cv_lr.best_score_,4))\n",
    "\n",
    "\n",
    "lr_res, lr_thr = report_model(\"LogReg\", best_lr, X_train, y_train, X_val, y_val, X_test, y_test, tune_threshold=True)\n",
    "print(lr_res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a859b778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best RF params: {'clf__max_depth': None, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1}  CV(AP): 0.6203\n",
      "              model  accuracy  precision    recall        f1   roc_auc  \\\n",
      "train  RandomForest  0.993982   0.974900  1.000000  0.987290  1.000000   \n",
      "val    RandomForest  0.772622   0.463602  0.683616  0.552511  0.829634   \n",
      "test   RandomForest  0.747731   0.492877  0.633700  0.554487  0.779067   \n",
      "\n",
      "       pr_auc(AP)     brier       mcc    tp   fp    tn   fn  threshold  \n",
      "train    1.000000  0.017714  0.983485  1942   50  6317    0       0.25  \n",
      "val      0.614031  0.118739  0.421332   121  140   545   56       0.25  \n",
      "test     0.575459  0.148896  0.388223   173  178   651  100       0.25  \n"
     ]
    }
   ],
   "source": [
    "pipe_rf = Pipeline([\n",
    "   (\"prep\", preprocessor),\n",
    "   (\"clf\", RandomForestClassifier(\n",
    "       n_estimators=400, n_jobs=-1, class_weight=\"balanced\", random_state=42\n",
    "   )),\n",
    "])\n",
    "param_rf = {\n",
    "   \"clf__max_depth\": [None, 10, 20],\n",
    "   \"clf__min_samples_leaf\": [1, 3, 5],\n",
    "   \"clf__max_features\": [\"sqrt\", 0.5, None],\n",
    "}\n",
    "cv_rf = GridSearchCV(\n",
    "   pipe_rf, param_grid=param_rf, cv=GroupKFold(n_splits=5),\n",
    "   scoring=\"average_precision\", n_jobs=-1, verbose=0\n",
    ")\n",
    "cv_rf.fit(X_train, y_train, groups=g_train)\n",
    "best_rf = cv_rf.best_estimator_\n",
    "print(\"\\nBest RF params:\", cv_rf.best_params_, \" CV(AP):\", round(cv_rf.best_score_,4))\n",
    "\n",
    "\n",
    "rf_res, rf_thr = report_model(\"RandomForest\", best_rf, X_train, y_train, X_val, y_val, X_test, y_test, tune_threshold=True)\n",
    "print(rf_res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649305d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best MLP params: {'clf__alpha': 0.001, 'clf__hidden_layer_sizes': (128, 64)}  CV(AP): 0.6232\n",
      "      model  accuracy  precision    recall        f1   roc_auc  pr_auc(AP)  \\\n",
      "train   MLP  0.812492   0.578947  0.725026  0.643804  0.871259    0.733035   \n",
      "val     MLP  0.790023   0.490741  0.598870  0.539440  0.809097    0.637196   \n",
      "test    MLP  0.768603   0.528125  0.619048  0.569983  0.800921    0.595532   \n",
      "\n",
      "          brier       mcc    tp    fp    tn   fn  threshold  \n",
      "train  0.111069  0.524762  1408  1024  5343  534       0.25  \n",
      "val    0.116764  0.408551   106   110   575   71       0.25  \n",
      "test   0.146306  0.415492   169   151   678  104       0.25  \n"
     ]
    }
   ],
   "source": [
    "pipe_mlp = Pipeline([\n",
    "   (\"prep\", preprocessor),\n",
    "   (\"clf\", MLPClassifier(\n",
    "       hidden_layer_sizes=(128, ),\n",
    "       activation=\"relu\", solver=\"adam\", max_iter=50, random_state=42,\n",
    "       early_stopping=True, n_iter_no_change=5, validation_fraction=0.1\n",
    "   )),\n",
    "])\n",
    "param_mlp = {\n",
    "   \"clf__hidden_layer_sizes\": [(64,), (128,), (128,64)],\n",
    "   \"clf__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "cv_mlp = GridSearchCV(\n",
    "   pipe_mlp, param_grid=param_mlp, cv=GroupKFold(n_splits=5),\n",
    "   scoring=\"average_precision\", n_jobs=-1, verbose=0\n",
    ")\n",
    "cv_mlp.fit(X_train, y_train, groups=g_train)\n",
    "best_mlp = cv_mlp.best_estimator_\n",
    "print(\"\\nBest MLP params:\", cv_mlp.best_params_, \" CV(AP):\", round(cv_mlp.best_score_,4))\n",
    "\n",
    "\n",
    "mlp_res, mlp_thr = report_model(\"MLP\", best_mlp, X_train, y_train, X_val, y_val, X_test, y_test, tune_threshold=True)\n",
    "print(mlp_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8b54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present result frames: {'dummy_res': True, 'lr_res': True, 'rf_res': True, 'mlp_res': True}\n",
      "\n",
      "== Validation Summary ==\n",
      "              pr_auc(AP)        f1    recall\n",
      "model                                       \n",
      "MLP             0.637196  0.539440  0.598870\n",
      "RandomForest    0.614031  0.552511  0.683616\n",
      "LogReg          0.582585  0.504348  0.491525\n",
      "Dummy           0.207909  0.230971  0.248588\n"
     ]
    }
   ],
   "source": [
    "# ==== เลือกผู้ชนะจากผลบน validation (robust) ====\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ตรวจว่าบล็อกผลลัพธ์ใดรันสำเร็จแล้วบ้าง\n",
    "present = {vn: isinstance(globals().get(vn), pd.DataFrame)\n",
    "          for vn in [\"dummy_res\",\"lr_res\",\"rf_res\",\"mlp_res\"]}\n",
    "print(\"present result frames:\", present)\n",
    "\n",
    "\n",
    "frames = []\n",
    "for label, varname in [\n",
    "   (\"Dummy\",        \"dummy_res\"),\n",
    "   (\"LogReg\",       \"lr_res\"),\n",
    "   (\"RandomForest\", \"rf_res\"),\n",
    "   (\"MLP\",          \"mlp_res\"),\n",
    "]:\n",
    "   df_var = globals().get(varname)\n",
    "   if isinstance(df_var, pd.DataFrame) and \"val\" in df_var.index:\n",
    "       frames.append(df_var.loc[[\"val\"]].copy())\n",
    "\n",
    "\n",
    "if not frames:\n",
    "   raise RuntimeError(\n",
    "       \"No model validation results found. \"\n",
    "       \"Make sure you ran the training cells that produce dummy_res/lr_res/rf_res/mlp_res.\"\n",
    "   )\n",
    "\n",
    "\n",
    "summary_val = pd.concat(frames, axis=0)\n",
    "summary_val = summary_val.set_index(\"model\", drop=True)\n",
    "\n",
    "\n",
    "# ตัด Dummy ออก (ถ้ามี) แล้วเลือกผู้ชนะ โดยเน้น PR-AUC -> F1 -> Recall\n",
    "summary_val_no_dummy = summary_val.drop(index=\"Dummy\", errors=\"ignore\")\n",
    "\n",
    "\n",
    "# ถ้าเผลอลบจนว่าง (เช่นมีแต่ Dummy จริง ๆ) ให้ fallback กลับไปใช้ summary_val\n",
    "candidates = summary_val_no_dummy if len(summary_val_no_dummy) > 0 else summary_val\n",
    "\n",
    "\n",
    "winner_name = candidates.sort_values(\n",
    "   by=[\"pr_auc(AP)\", \"f1\", \"recall\"], ascending=False\n",
    ").index[0]\n",
    "\n",
    "\n",
    "print(\"\\n== Validation Summary ==\")\n",
    "print(summary_val.loc[:, [\"pr_auc(AP)\", \"f1\", \"recall\"]].sort_values(\"pr_auc(AP)\", ascending=False))\n",
    "\n",
    "\n",
    "# map เฉพาะโมเดลที่มีจริง\n",
    "winners = {}\n",
    "if \"LogReg\"       in summary_val.index: winners[\"LogReg\"]       = (best_lr, lr_thr)\n",
    "if \"RandomForest\" in summary_val.index: winners[\"RandomForest\"] = (best_rf, rf_thr)\n",
    "if \"MLP\"          in summary_val.index: winners[\"MLP\"]          = (best_mlp, mlp_thr)\n",
    "\n",
    "\n",
    "best_model_template, best_thr_val = winners.get(winner_name, (None, None))\n",
    "if best_model_template is None:\n",
    "   raise RuntimeError(f\"Winner '{winner_name}' not available in winners map. Check which models were trained.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38075166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== FINAL on TEST using MLP ==\n",
      "      accuracy  precision    recall        f1   roc_auc  pr_auc(AP)     brier  \\\n",
      "test  0.783122      0.585  0.428571  0.494715  0.794912    0.597636  0.146171   \n",
      "\n",
      "           mcc   tp  fp   tn   fn  threshold  \n",
      "test  0.367884  117  83  746  156       0.35  \n"
     ]
    }
   ],
   "source": [
    "def concat_X(*dfs):\n",
    "   return pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "X_trval = concat_X(X_train, X_val)\n",
    "y_trval = np.concatenate([y_train, y_val])\n",
    "\n",
    "\n",
    "best_model_final = best_model_template\n",
    "best_model_final.fit(X_trval, y_trval)\n",
    "\n",
    "\n",
    "final_thr, _ = find_best_threshold(best_model_final, X_val, y_val, target_metric=\"f1\")\n",
    "final_res = get_scores(best_model_final, X_test, y_test, threshold=final_thr)\n",
    "\n",
    "\n",
    "print(f\"\\n== FINAL on TEST using {winner_name} ==\")\n",
    "print(pd.DataFrame([final_res], index=[\"test\"]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65cc614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST(TEST) classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8271    0.8999    0.8619       829\n",
      "           1     0.5850    0.4286    0.4947       273\n",
      "\n",
      "    accuracy                         0.7831      1102\n",
      "   macro avg     0.7060    0.6642    0.6783      1102\n",
      "weighted avg     0.7671    0.7831    0.7710      1102\n",
      "\n",
      "Confusion matrix [0,1]:\n",
      " [[746  83]\n",
      " [156 117]]\n"
     ]
    }
   ],
   "source": [
    "def print_full_report(name, clf, X, y, threshold):\n",
    "   if hasattr(clf, \"predict_proba\"):\n",
    "       s = clf.predict_proba(X)[:, 1]\n",
    "   else:\n",
    "       d = clf.decision_function(X)\n",
    "       s = (d - d.min()) / (d.max() - d.min() + 1e-9)\n",
    "   y_pred = (s >= threshold).astype(int)\n",
    "   print(f\"\\n{name} classification_report:\\n\", classification_report(y, y_pred, digits=4))\n",
    "   print(\"Confusion matrix [0,1]:\\n\", confusion_matrix(y, y_pred, labels=[0,1]))\n",
    "\n",
    "\n",
    "print_full_report(\"BEST(TEST)\", best_model_final, X_test, y_test, final_thr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a2b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner_name = MLP\n",
      "MLP has no direct coefficients — running permutation importance...\n",
      "\n",
      "Top 15 permutation importances (input level):\n",
      "log1p_amount 0.33173\n",
      "channel 0.223934\n",
      "tx_code 0.200926\n",
      "description_text 0.172914\n",
      "debit_amount 0.088082\n",
      "credit_amount 0.041641\n",
      "balance_amount 0.032793\n",
      "abs_amount 0.031147\n",
      "hour 0.024019\n",
      "net_amount 0.022721\n",
      "is_weekend 0.007353\n",
      "month 0.006993\n",
      "dayofweek 0.003979\n",
      "year -0.000762\n",
      "day -0.01438\n",
      "Requirement already satisfied: openpyxl in /Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Users/wysuttida/pattern-project/AI-Statement-IntelliScan/.venv/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Split sizes: 8309 862 1102\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names(prep: ColumnTransformer):\n",
    "   names = []\n",
    "   for name, trans, cols in prep.transformers_:\n",
    "       if name == \"num\":\n",
    "           # numeric columns\n",
    "           names.extend(cols)\n",
    "       elif name == \"cat\":\n",
    "           ohe = trans\n",
    "           try:\n",
    "               names.extend(list(ohe.get_feature_names_out(cols)))\n",
    "           except:\n",
    "               names.extend(cols)\n",
    "       elif name == \"txt\":\n",
    "           tfidf = trans\n",
    "           try:\n",
    "               names.extend(list(tfidf.get_feature_names_out()))\n",
    "           except:\n",
    "               names.append(\"tfidf_features\")\n",
    "   return names\n",
    "\n",
    "\n",
    "print(\"winner_name =\", winner_name)\n",
    "if winner_name == \"LogReg\":\n",
    "   coefs = best_model_final.named_steps[\"clf\"].coef_[0]\n",
    "   feat_names = get_feature_names(best_model_final.named_steps[\"prep\"])\n",
    "   top_pos_idx = np.argsort(coefs)[-15:][::-1]\n",
    "   top_neg_idx = np.argsort(coefs)[:15]\n",
    "   print(\"\\nTop + coefficients:\")\n",
    "   for i in top_pos_idx: print(feat_names[i], round(coefs[i], 4))\n",
    "   print(\"\\nTop - coefficients:\")\n",
    "   for i in top_neg_idx: print(feat_names[i], round(coefs[i], 4))\n",
    "\n",
    "\n",
    "elif winner_name == \"RandomForest\":\n",
    "   rf = best_model_final.named_steps[\"clf\"]\n",
    "   feat_names = get_feature_names(best_model_final.named_steps[\"prep\"])\n",
    "   importances = getattr(rf, \"feature_importances_\", None)\n",
    "   if importances is not None:\n",
    "       idx = np.argsort(importances)[-20:][::-1]\n",
    "       print(\"\\nTop 20 RF importances:\")\n",
    "       for i in idx: print(feat_names[i], round(importances[i], 4))\n",
    "\n",
    "\n",
    "else:\n",
    "   print(f\"{winner_name} has no direct coefficients — running permutation importance...\")\n",
    "   r = permutation_importance(\n",
    "       best_model_final, X_test, y_test,\n",
    "       n_repeats=5, random_state=42, n_jobs=-1, scoring=\"average_precision\"\n",
    "   )\n",
    "   base_names = list(X_test.columns)\n",
    "   idx = np.argsort(r.importances_mean)[-15:][::-1]\n",
    "   print(\"\\nTop 15 permutation importances (input level):\")\n",
    "   for i in idx:\n",
    "       print(base_names[i], round(r.importances_mean[i], 6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%pip install openpyxl\n",
    "\n",
    "\n",
    "# ==== PRELUDE: load + preprocess + split (run this BEFORE the Keras cell) ====\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "DATA_DIR = \"/Users/wysuttida/pattern-project/mockdata_transaction\"  # ปรับได้ถ้าตำแหน่งต่าง\n",
    "\n",
    "\n",
    "def load_all_statements(data_dir=DATA_DIR):\n",
    "   paths = sorted(glob.glob(os.path.join(data_dir, \"*.csv\")) + glob.glob(os.path.join(data_dir, \"*.xlsx\")))\n",
    "   if not paths:\n",
    "       raise FileNotFoundError(f\"ไม่พบไฟล์ใน {data_dir}\")\n",
    "   dfs = []\n",
    "   for p in paths:\n",
    "       ext = os.path.splitext(p)[1].lower()\n",
    "       df = pd.read_excel(p, engine=\"openpyxl\") if ext == \".xlsx\" else pd.read_csv(p)\n",
    "       df[\"file_id\"] = os.path.basename(p)\n",
    "       dfs.append(df)\n",
    "   return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "   df = df.copy()\n",
    "   # datetime\n",
    "   df[\"tx_datetime\"] = pd.to_datetime(df[\"tx_datetime\"], errors=\"coerce\")\n",
    "   df = df.dropna(subset=[\"tx_datetime\"]).sort_values([\"file_id\",\"tx_datetime\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "   # split code/channel\n",
    "   sp = df[\"code_channel_raw\"].astype(str).str.split(\"/\", n=1, expand=True)\n",
    "   df[\"tx_code\"] = sp[0].str.strip()\n",
    "   df[\"channel\"] = sp[1].str.strip() if sp.shape[1] > 1 else \"\"\n",
    "\n",
    "\n",
    "   # numeric (ensure present)\n",
    "   for col in [\"debit_amount\",\"credit_amount\",\"balance_amount\"]:\n",
    "       if col not in df.columns:\n",
    "           df[col] = 0.0\n",
    "   for col in [\"debit_amount\",\"credit_amount\",\"balance_amount\"]:\n",
    "       df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "   # engineered\n",
    "   df[\"net_amount\"] = df[\"credit_amount\"] - df[\"debit_amount\"]\n",
    "   df[\"abs_amount\"] = df[\"debit_amount\"].abs() + df[\"credit_amount\"].abs()\n",
    "   df[\"log1p_amount\"] = np.log1p(df[\"abs_amount\"])\n",
    "\n",
    "\n",
    "   # time features\n",
    "   dt = df[\"tx_datetime\"]\n",
    "   df[\"hour\"] = dt.dt.hour\n",
    "   df[\"dayofweek\"] = dt.dt.dayofweek\n",
    "   df[\"is_weekend\"] = (df[\"dayofweek\"] >= 5).astype(int)\n",
    "   df[\"day\"] = dt.dt.day\n",
    "   df[\"month\"] = dt.dt.month\n",
    "   df[\"year\"] = dt.dt.year\n",
    "\n",
    "\n",
    "   # text & label\n",
    "   df[\"description_text\"] = df[\"description_text\"].astype(str).fillna(\"\")\n",
    "   df[\"fraud_label\"] = df[\"fraud_label\"].astype(int)\n",
    "   return df\n",
    "\n",
    "\n",
    "# load + preprocess\n",
    "df = preprocess_dataframe(load_all_statements(DATA_DIR))\n",
    "\n",
    "\n",
    "# group split (80/10/10) by file_id\n",
    "groups = df[\"file_id\"].values\n",
    "y_all = df[\"fraud_label\"].values\n",
    "\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "train_idx, temp_idx = next(gss.split(df, y_all, groups=groups))\n",
    "\n",
    "\n",
    "groups_temp = groups[temp_idx]\n",
    "y_temp = y_all[temp_idx]\n",
    "gss2 = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=42)\n",
    "val_rel, test_rel = next(gss2.split(df.iloc[temp_idx], y_temp, groups=groups_temp))\n",
    "\n",
    "\n",
    "val_idx = temp_idx[val_rel]\n",
    "test_idx = temp_idx[test_rel]\n",
    "\n",
    "\n",
    "def take(idx): return df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_df = take(train_idx)\n",
    "val_df   = take(val_idx)\n",
    "test_df  = take(test_idx)\n",
    "\n",
    "\n",
    "# y สำหรับประเมิน/คำนวณ threshold\n",
    "y_train = train_df[\"fraud_label\"].values\n",
    "y_val   = val_df[\"fraud_label\"].values\n",
    "y_test  = test_df[\"fraud_label\"].values\n",
    "\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c2398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved scaler/tfidf/vocabs to /Users/wysuttida/pattern-project/API-Statement-IntelliScan\n"
     ]
    }
   ],
   "source": [
    "# ==== 16A) Fit external preprocessors & save artifacts ====\n",
    "import os, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "OUT_DIR = \"/Users/wysuttida/pattern-project/API-Statement-IntelliScan\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"debit_amount\",\"credit_amount\",\"balance_amount\",\n",
    "    \"net_amount\",\"abs_amount\",\"log1p_amount\",\n",
    "    \"hour\",\"dayofweek\",\"is_weekend\",\"day\",\"month\",\"year\"\n",
    "]\n",
    "TEXT_FEATURE = \"description_text\"\n",
    "\n",
    "# 1) Numeric scaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "scaler.fit(train_df[NUMERIC_FEATURES])\n",
    "\n",
    "# 2) Categorical vocab (one-hot ภายนอก)\n",
    "tx_vocab = sorted(train_df[\"tx_code\"].astype(str).unique().tolist())\n",
    "ch_vocab = sorted(train_df[\"channel\"].astype(str).unique().tolist())\n",
    "tx_index = {t:i for i,t in enumerate(tx_vocab)}\n",
    "ch_index = {t:i for i,t in enumerate(ch_vocab)}\n",
    "\n",
    "# 3) Text TF-IDF (ตั้งค่าให้ใกล้เคียงของเดิม)\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.95,\n",
    "    strip_accents=\"unicode\"\n",
    ")\n",
    "tfidf.fit(train_df[TEXT_FEATURE].astype(str))\n",
    "\n",
    "# Save preprocessors\n",
    "joblib.dump(scaler, f\"{OUT_DIR}/pre_scaler.joblib\")\n",
    "joblib.dump(tfidf,  f\"{OUT_DIR}/pre_tfidf.joblib\")\n",
    "with open(f\"{OUT_DIR}/pre_categ_vocab.json\", \"w\") as f:\n",
    "    json.dump({\"tx_vocab\": tx_vocab, \"ch_vocab\": ch_vocab}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] Saved scaler/tfidf/vocabs to\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59f7ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: train (8309, 201) val (862, 201) test (1102, 201)\n"
     ]
    }
   ],
   "source": [
    "# ==== 16B) Transform DataFrames to feature matrices (numeric + one-hot + tfidf) ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "def _one_hot_from_vocab(series_str: pd.Series, index_map: dict, vocab_size: int):\n",
    "    \"\"\" สร้าง one-hot (CSR) จาก vocab ที่ฟิตไว้ (ตัวนอก vocab จะเป็นแถว zero) \"\"\"\n",
    "    arr = series_str.astype(str).map(index_map).to_numpy()\n",
    "    N = len(arr)\n",
    "    rows = np.arange(N, dtype=np.int64)\n",
    "    mask = ~pd.isna(arr)\n",
    "    cols = arr[mask].astype(np.int64)\n",
    "    data = np.ones(mask.sum(), dtype=np.float32)\n",
    "    return sp.csr_matrix((data, (rows[mask], cols)), shape=(N, vocab_size), dtype=np.float32)\n",
    "\n",
    "def transform_df_to_X(df: pd.DataFrame, scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab):\n",
    "    # numeric → scale → csr\n",
    "    X_num = scaler.transform(df[NUMERIC_FEATURES]).astype(np.float32)\n",
    "    X_num = sp.csr_matrix(X_num)\n",
    "\n",
    "    # categorical → one-hot csr\n",
    "    X_tx = _one_hot_from_vocab(df[\"tx_code\"].astype(str), tx_index, len(tx_vocab))\n",
    "    X_ch = _one_hot_from_vocab(df[\"channel\"].astype(str), ch_index, len(ch_vocab))\n",
    "\n",
    "    # text → tfidf csr\n",
    "    X_txt = tfidf.transform(df[TEXT_FEATURE].astype(str)).astype(np.float32)\n",
    "\n",
    "    # hstack → csr\n",
    "    X = sp.hstack([X_num, X_tx, X_ch, X_txt], format=\"csr\", dtype=np.float32)\n",
    "    return X\n",
    "\n",
    "# แปลง train/val/test\n",
    "X_train_ext = transform_df_to_X(train_df, scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "X_val_ext   = transform_df_to_X(val_df,   scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "X_test_ext  = transform_df_to_X(test_df,  scaler, tfidf, tx_index, ch_index, tx_vocab, ch_vocab)\n",
    "\n",
    "print(\"Shapes:\",\n",
    "      \"train\", X_train_ext.shape,\n",
    "      \"val\",   X_val_ext.shape,\n",
    "      \"test\",  X_test_ext.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3389877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'accuracy': 0.7577132486388385, 'precision': 0.5090361445783133, 'recall': 0.6190476190476191, 'f1': 0.5586776859504132, 'roc_auc': 0.8074735879319714, 'pr_auc(AP)': 0.6014701791949221, 'threshold': 0.5499999999999999}\n",
      "[OK] Saved model.h5 and model_meta.json at /Users/wysuttida/pattern-project/API-Statement-IntelliScan\n"
     ]
    }
   ],
   "source": [
    "# ==== 16C) Keras model (no preprocessing inside) + save .h5 ====\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# แปลงเป็น dense (ถ้าเมมไม่พอ ลดฟีเจอร์ก่อน)\n",
    "Xtr = X_train_ext.toarray()\n",
    "Xva = X_val_ext.toarray()\n",
    "Xte = X_test_ext.toarray()\n",
    "\n",
    "# Labels\n",
    "y_tr = y_train.astype(np.float32)\n",
    "y_va = y_val.astype(np.float32)\n",
    "y_te = y_test.astype(np.float32)\n",
    "\n",
    "# Class weights → sample_weight (เหมือนเดิม)\n",
    "classes = np.array([0,1])\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "w0, w1 = float(cw[0]), float(cw[1])\n",
    "sw_tr = np.where(y_tr == 1, w1, w0).astype(np.float32)\n",
    "\n",
    "# Build model (input = เวกเตอร์ฟีเจอร์รวม)\n",
    "inp = layers.Input(shape=(Xtr.shape[1],), name=\"X\")\n",
    "x = layers.Dense(128, activation=\"relu\")(inp)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inp, out)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",\n",
    "              metrics=[tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                       tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                       tf.keras.metrics.Recall(name=\"recall\")])\n",
    "\n",
    "# Train\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor=\"val_auc\", mode=\"max\")\n",
    "history = model.fit(\n",
    "    Xtr, y_tr,\n",
    "    validation_data=(Xva, y_va),\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    sample_weight=sw_tr,\n",
    "    callbacks=[es],\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# เลือก threshold จาก validation เพื่อ maximize F1\n",
    "val_scores = model.predict(Xva, verbose=0).ravel()\n",
    "ths = np.linspace(0.05, 0.95, 19)\n",
    "def f1_at(t):\n",
    "    return f1_score(y_va, (val_scores >= t).astype(int), zero_division=0)\n",
    "best_thr = float(ths[np.argmax([f1_at(t) for t in ths])])\n",
    "\n",
    "# ประเมินบน test\n",
    "test_scores = model.predict(Xte, verbose=0).ravel()\n",
    "pred_test = (test_scores >= best_thr).astype(int)\n",
    "final_metrics = {\n",
    "    \"accuracy\": float((pred_test == y_te).mean()),\n",
    "    \"precision\": float(precision_score(y_te, pred_test, zero_division=0)),\n",
    "    \"recall\": float(recall_score(y_te, pred_test, zero_division=0)),\n",
    "    \"f1\": float(f1_score(y_te, pred_test, zero_division=0)),\n",
    "    \"roc_auc\": float(roc_auc_score(y_te, test_scores)),\n",
    "    \"pr_auc(AP)\": float(average_precision_score(y_te, test_scores)),\n",
    "    \"threshold\": best_thr,\n",
    "}\n",
    "print(\"Test metrics:\", final_metrics)\n",
    "\n",
    "# เซฟเป็น .h5 ได้แล้ว (ไม่มี StringLookup/TextVectorization ภายในโมเดล)\n",
    "model.save(f\"{OUT_DIR}/model.h5\")\n",
    "with open(f\"{OUT_DIR}/model_meta.json\", \"w\") as f:\n",
    "    json.dump({\"threshold\": best_thr, \"metrics_test\": final_metrics}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] Saved model.h5 and model_meta.json at\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI IntelliScan (tf)",
   "language": "python",
   "name": "ai-intelliscan-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
